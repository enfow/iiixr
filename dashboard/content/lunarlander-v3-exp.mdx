---
title: "LunarLander-v3 Experiment"
date: 2025-06-29
type: "experiment"
---

# LunarLander-v3 Experiment

본 실험은 강화학습의 세 가지 대표적인 알고리즘인 **PPO, RainbowDQN, SAC(Discrete)** 를 **LunarLander-v3** 환경에서 구현하고 성능을 비교 분석하는 것을 목적으로 한다. 각 알고리즘은 PyTorch를 사용하여 직접 구현하였고, 최대한 논문의 알고리즘과 동일하게 구현하기 위해 노력했다. 또한 독립적인 실험 환경을 위해 Docker 를 사용하여 실험을 진행했다.

## 실험 모델

- **DDQN(Double Deep Q-Network)**: Value based model 의 Baseline
  - [PAPER](<https://arxiv.org/abs/1509.06461>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/ddqn_trainer.py>)
- **RainbowDQN**: DQN 의 성능을 개선하는 여러 알고리즘들을 통합 적용한 알고리즘
  - [REVIEW](./rainbow_dqn) | [PAPER](<https://arxiv.org/abs/1706.02275>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/rainbow_dqn_trainer.py>)
- **PPO(Discrete)**: Policy-based, On-Policy 알고리즘(Trust Region)
  - [REVIEW](./ppo) | [PAPER](<https://arxiv.org/abs/1707.06347>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_ppo_trainer.py>)
- **SAC(Discrete)**: Policy-based, Off-Policy 알고리즘(Actor-Critic, Maximum Entropy)
  - [REVIEW](<./sac>) | [PAPER](<https://arxiv.org/abs/1910.07207>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_sac_trainer.py>)

## 실험 환경

[LunarLander-v3](./lunarlander)

- 8차원의 State space 
- 4차원의 Discrete Action space

## 실험 결과

상대적으로 간단한 환경에서 진행한 실험인 만큼 과제에서 요구하는 `PPO`, `SAC`, `Rainbow DQN` 세 알고리즘 모두 `Return=250` 을 넘는 성능을 보여주었을 뿐만 아니라, 성능 비교를 위해 구현한 `DDQN` 으로도 `Episode=1000` 내에 빠르게 높은 성능에 도달하였다.

### DDQN

[IMPLEMENTATION (DDQN)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/ddqn_trainer.py>)

Value Based mdoel 알고리즘의 Baseline 성능을 확인하기 위해 DQN이 가지는 과대 추정 문제를 해결하기 위해 두 개의 네트워크를 사용하는 DDQN 에 대한 실험을 진행했다.

![DDQN agent on LunarLander-v3](/experiments/lunarlander/ddqn/training.png)

`LunarLander-v3` 에서 약 750 Episode 에서 `Return=250` 점을 기록했다.

<div align="center">
  <img src="/experiments/lunarlander/ddqn/demo.gif" alt="DDQN agent on LunarLander-v3" />
</div>

### RainbowDQN

[IMPLEMENTATION (RAINBOW-DQN)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/rainbow_dqn_trainer.py>)

DQN 계열의 알고리즘에서 적용 가능한 다양한 알고리즘들을 적용한 Rainbow DQN 이 최고 성능(Maximum Return) `332.27`으로 가장 높은 성능을 보여주었다.

![RainbowDQN agent on LunarLander-v3](/experiments/lunarlander/rainbow_dqn/training.png)

<div align="center">
  <img src="/experiments/lunarlander/rainbow_dqn/demo.gif" alt="RainbowDQN agent on LunarLander-v3" />
</div>

### PPO

[IMPLEMENTATION (DISCRETE)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_ppo_trainer.py>)

PPO 알고리즘 또한 동일한 환경에서 높은 성능을 기록했다. 특히 On-Policy 알고리즘의 특징을 반영하듯 매우 빠른 수렴 속도와 안정적인 학습 경향성을 보여주었다.

![PPO agent on LunarLander-v3](/experiments/lunarlander/ppo/training.png)

<div align="center">
  <img src="/experiments/lunarlander/ppo/demo.gif" alt="PPO agent on LunarLander-v3" />
</div>

### SAC(Discrete)

[IMPLEMENTATION (DISCRETE)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_sac_trainer.py>)

Discrete Action Space 에 맞게 구현된 SAC 알고리즘 또한 앞선 PPO, Rainbow DQN 알고리즘들과 유사한 성능을 보여주었다. PPO와 비교해 볼 때 Return 의 Variance 가 더 큰 것을 확인할 수 있다.

![SAC(Discrete) agent on LunarLander-v3](/experiments/lunarlander/discrete_sac/training.png)

<div align="center">
  <img src="/experiments/lunarlander/discrete_sac/demo.gif" alt="SAC(Discrete) agent on LunarLander-v3" />
</div>

## 실험 결과

`LunarLandver-v3` 환경을 해결하는 데에 `Rainbow DQN` 이 가장 높은 성능을 기록했다는 점에서, 복잡한 알고리즘 조합이 단순한 환경에서도 높은 성능을 보인다는 것을 확인했다. `PPO` 알고리즘은 수렴 속도가 가장 빠르고, 학습이 가장 안정적이었다. 이는 전형적인 On-Policy 계열의 알고리즘의 특성이 이번 실험에서도 확인되었다고 할 수 있다.

### 한계점 및 개선방안

이번 실험에서는 하나의 시드(`seed=42`)로 설정하고 1회 실험을 진행했다. 따라서 실험 결과에 대한 통계적 유의성이 높지 않다. 5~10 개의 시드로 복수 실험하고, 그 성능을 비교해 볼 필요가 있다.

## Appendix: Hyperparameter

### DDQN

```json
{
  "model": {
    "model": "ddqn",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "per",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1,
    "per_n_steps": 3
  },
  "seed": 42,
  "episodes": 1000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": 8,
  "action_dim": 4,
  "is_discrete": true,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "target_update_interval": 8000,
  "start_steps": 10000,
  "eps_start": 1.0,
  "eps_end": 0.01,
  "eps_decay": 50000
}
```

### RainbowDQN

```json
{
  "model": {
    "model": "rainbow_dqn",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "per",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1
  },
  "seed": 42,
  "episodes": 5000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": 8,
  "action_dim": 4,
  "is_discrete": true,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "target_update_interval": 8000,
  "n_steps": 3,
  "n_atoms": 51,
  "v_min": -400.0,
  "v_max": 400.0,
  "start_steps": 10000
}
```

### PPO

```json
{
  "model": {
    "model": "ppo",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "default",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1
  },
  "seed": 42,
  "episodes": 5000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": 8,
  "action_dim": 4,
  "is_discrete": true,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "n_transactions": 5000,
  "ppo_epochs": 5,
  "clip_eps": 0.2,
  "normalize_advantages": false,
  "entropy_coef": 0.2
}
```

### SAC(Discrete)

```json
{
  "model": {
    "model": "discrete_sac",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "default",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1,
    "per_n_steps": 3
  },
  "seed": 42,
  "episodes": 5000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": null,
  "action_dim": null,
  "is_discrete": null,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "tau": 0.005,
  "entropy_coef": 0.98,
  "start_steps": 20000,
  "target_update_interval": 1
}
```
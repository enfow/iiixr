---
title: "LunarLander-v3 Experiment"
date: 2025-06-29
type: "experiment"
---

# LunarLander-v3 Experiment

본 실험은 강화학습의 세 가지 대표적인 알고리즘인 **PPO, RainbowDQN, SAC(Discrete)** 를 **LunarLander-v3** 환경에서 구현하고 성능을 비교 분석하는 것을 목적으로 한다. 각 알고리즘은 PyTorch를 사용하여 직접 구현하였고, 최대한 논문의 알고리즘과 동일하게 구현하기 위해 노력했다. 또한 독립적인 실험 환경을 위해 Docker 를 사용하여 실험을 진행했다.

## 실험 모델

- **DDQN(Double Deep Q-Network)**: Value based model 의 Baseline
  - [PAPER](<https://arxiv.org/abs/1509.06461>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/ddqn_trainer.py>)
- **C51(Categorial DQN)**: Rainbow DQN 의 distributional RL 알고리즘의 검증 목적으로 구현
  - [PAPER](<https://arxiv.org/abs/1707.06887>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/c51_trainer.py>)
- **RainbowDQN**: DQN 의 성능을 개선하는 여러 알고리즘들을 통합 적용한 알고리즘
  - [REVIEW](./rainbow_dqn) | [PAPER](<https://arxiv.org/abs/1706.02275>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/rainbow_dqn_trainer.py>)
- **PPO(Discrete)**: Policy-based, On-Policy 알고리즘(Trust Region)
  - [REVIEW](./ppo) | [PAPER](<https://arxiv.org/abs/1707.06347>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_ppo_trainer.py>)
- **SAC(Discrete)**: Policy-based, Off-Policy 알고리즘(Actor-Critic, Entropy)
  - [REVIEW](<./sac>) | [PAPER](<https://arxiv.org/abs/1910.07207>) | [CODE](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_sac_trainer.py>)

## 실험 환경

[LunarLander-v3](./lunarlander)

- 8차원의 State space 
- 4차원의 Discrete Action space

## 실험 결과

상대적으로 간단한 환경에서 진행한 실험인 만큼 모든 알고리즘이 높은 점수를 보여주었다.

### DDQN

Value Based mdoel 알고리즘의 Baseline 성능을 확인하기 위해 DQN이 가지는 과대 추정 문제를 해결하기 위해 두 개의 네트워크를 사용하는 DDQN 에 대한 실험을 진행했다.

![DDQN agent on LunarLander-v3](/experiments/lunarlander/ddqn/training.png)

`LunarLander-v3` 에서 약 750 Episode 에서 250 점을 기록했다.

<div align="center">
  <img src="/experiments/lunarlander/ddqn/demo.gif" alt="DDQN agent on LunarLander-v3" />
</div>


### C51

2000 Episode 를 기준으로 C51 은 학습에 성공하지 못했다.

![C51 agent on LunarLander-v3](/experiments/lunarlander/c51/training.png)

<div align="center">
  <img src="/experiments/lunarlander/c51/demo.gif" alt="C51 agent on LunarLander-v3" />
</div>

### RainbowDQN

DQN 계열의 알고리즘에서 적용 가능한 다양한 알고리즘들을 적용한 Rainbow DQN 이 최고 성능 332.27으로 가장 높은 성능을 보여주었다.

![RainbowDQN agent on LunarLander-v3](/experiments/lunarlander/rainbow_dqn/training.png)

<div align="center">
  <img src="/experiments/lunarlander/rainbow_dqn/demo.gif" alt="RainbowDQN agent on LunarLander-v3" />
</div>

### PPO

PPO 알고리즘 또한 동일한 환경에서 높은 성능을 보여주었고, 특히 On-Policy 알고리즘의 특징을 반영하듯 매우 빠른 수렴 속도와 안정적인 학습 경향성을 보여주었다.

![PPO agent on LunarLander-v3](/experiments/lunarlander/ppo/training.png)

<div align="center">
  <img src="/experiments/lunarlander/ppo/demo.gif" alt="PPO agent on LunarLander-v3" />
</div>

### SAC(Discrete)


![SAC(Discrete) agent on LunarLander-v3](/experiments/lunarlander/sac/training.png)

<div align="center">
  <img src="/experiments/lunarlander/sac/demo.gif" alt="SAC(Discrete) agent on LunarLander-v3" />
</div>

## Conclusion

## Appendix: Hyperparameter

### DDQN

```json
{
  "model": {
    "model": "ddqn",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "per",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1,
    "per_n_steps": 3
  },
  "seed": 42,
  "episodes": 1000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": 8,
  "action_dim": 4,
  "is_discrete": true,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "target_update_interval": 8000,
  "start_steps": 10000,
  "eps_start": 1.0,
  "eps_end": 0.01,
  "eps_decay": 50000
}
```

### C51

```json
{
  "model": {
    "model": "c51",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "default",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1,
    "per_n_steps": 3
  },
  "seed": 42,
  "episodes": 2000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": null,
  "action_dim": null,
  "is_discrete": null,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "target_update_interval": 8000,
  "start_steps": 10000,
  "n_atoms": 51,
  "v_min": -400.0,
  "v_max": 400.0,
  "n_steps": 3,
  "eps_start": 1.0,
  "eps_end": 0.01,
  "eps_decay": 50000
}
```

### RainbowDQN

```json
{
  "model": {
    "model": "rainbow_dqn",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "per",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1
  },
  "seed": 42,
  "episodes": 5000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": 8,
  "action_dim": 4,
  "is_discrete": true,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "target_update_interval": 8000,
  "n_steps": 3,
  "n_atoms": 51,
  "v_min": -400.0,
  "v_max": 400.0,
  "start_steps": 10000
}
```

### PPO

```json
{
  "model": {
    "model": "ppo",
    "hidden_dim": 256,
    "n_layers": 3,
    "embedding_type": "fc"
  },
  "buffer": {
    "buffer_size": 1000000,
    "buffer_type": "default",
    "alpha": 0.6,
    "beta_start": 0.4,
    "beta_frames": 100000,
    "seq_len": 1
  },
  "seed": 42,
  "episodes": 5000,
  "max_steps": 1000,
  "lr": 0.0003,
  "gamma": 0.99,
  "batch_size": 256,
  "env": "LunarLander-v3",
  "state_dim": 8,
  "action_dim": 4,
  "is_discrete": true,
  "device": "cpu",
  "eval": true,
  "eval_period": 10,
  "eval_episodes": 10,
  "n_transactions": 5000,
  "ppo_epochs": 5,
  "clip_eps": 0.2,
  "normalize_advantages": false,
  "entropy_coef": 0.2
}
```

### SAC(Discrete)

```json

```
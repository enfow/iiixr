---
title: "SAC Algorithm"
date: 2024-01-16
type: "model"
---

 SAC(Soft Actor-Critic)는 off-policy actor-critic 계열의 알고리즘으로, reward를 최대화하는 동시에 policy의 entropy 또한 함께 최대화하는 방향으로 학습을 진행한다. Policy의 entropy를 높인다는 것은 특정 action을 극단적으로 선택할 가능성을 낮추어 학습 과정에서 exploration을 높이는 효과를 보인다.

$$
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T} \gamma^t \left(r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right)\right]
$$

 SAC는 value network, Q network, policy network를 갖는다. 여기서 value network는 Q network의 target value 계산에 사용되고, Q network는 critic의 역할을 한다. Policy는 Q 값을 높이면서 동시에 적절한 entropy를 유지하는 방향으로 업데이트된다. 이때 Q-value의 over estimation 문제가 학습 안정성에 큰 영향을 미치므로, Double Q learning 을 적용하고 있다. 여기서 $\alpha$ 는 temperature parameter 로, exploration 과 exploitation 간의 비율을 조절하는 계수이다. 

SAC를 구현하는 방법은 다양하다. 원래 버전은 별도의 value network를 사용하지만, 후속 연구에서는 value function 없이 double target Q network만 사용하는 방법도 제안되었다. 또한 원래 continuous action을 위해 설계되었지만, discrete 환경에 맞게 구현하는 방법도 있다.

### SAC Hyperparameter

- **tau**: Target network의 soft update를 위한 계수이다.  tau 값이 작을수록 target network가 천천히 업데이트되어 학습이 안정적이지만 느려지고, 클수록 빠르게 업데이트되지만 불안정해질 수 있다.
- **entropy_coef**: Temperature parameter α의 초기값 또는 고정값으로, exploration의 강도를 결정한다.
---
title: "SAC"
date: 2024-01-16
type: "model"
---

# Soft Actor-Critic(SAC)

[IMPLEMENTATION (ORIGINAL)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/sac_trainer.py>)| [IMPLEMENTATION (v2)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/sac_v2_trainer.py>) | [IMPLEMENTATION (DISCRETE)](<https://github.com/enfow/iiixr/blob/main/trainer/src/trainer/discrete_trainer.py>)

 **SAC**는 **Off-Policy Actor-Critic** 계열의 알고리즘으로, Reward를 최대화하는 동시에 Policy의 Entropy 또한 함께 최대화하는 방향으로 학습을 진행한다. Policy의 Entropy를 높인다는 것은 특정 Action을 극단적으로 선택할 가능성을 낮추는 것을 의미하며, 이는 학습 과정에서 exploration을 높이는 효과로 이어진다.

$$
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_{t=0}^{T} \gamma^t \left(r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right)\right]
$$

SAC는 그 구조상 value network, Q network, policy network를 갖는다. 여기서 value network는 Q network의 target value 계산에 사용되고, Q network는 critic의 역할을 한다. Policy는 Q 값을 높이면서 동시에 적절한 entropy를 유지하는 방향으로 업데이트된다. 이때 Q-value의 over estimation 문제가 학습 안정성에 큰 영향을 미치므로, Double Q learning 을 적용하고 있다. 여기서 $\alpha$ 는 temperature parameter 로, exploration 과 exploitation 간의 비율을 조절하는 계수이다[1]. 

SAC를 구현하는 방법은 다양하다. 원래 버전은 별도의 value network를 사용하지만, 후속 연구에서는 value function 없이 double target Q network만 사용하는 방법도 제안[2]되었다. 또한 원래 SAC는 Continuous Action을 갖는 환경을 위해 제안되었지만, Discrete 환경에 맞게 구현하는 방법[3]도 있다.

### Implementation

SAC는 변형이 많은 알고리즘인데, 원 알고리즘을 포함하여 총 세 가지 종류의 구현을 진행했다. 각각의 구현은 각자의 논문을 최대한 따라 구현하기 위해 노력했다. 

- **[Original SAC]** [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor(Haarnoja, et al. 2018)](<https://arxiv.org/pdf/1812.05905>)
- **[SAC v2]** [Soft Actor-Critic Algorithms and Applications(Haarnoja, et al. 2018)](<https://arxiv.org/pdf/1812.05905>)
  - Original SAC 와 달리 State Value Function(V) 이 생략되었다.
  - Double Target Q Network 가 도입되었다.
- **[DISCRETE SAC]** [Soft Actor-Critic for Discrete Action Settings(Petros Christodoulou, 2019)](<https://arxiv.org/abs/1910.07207>)
  - Discrete Action Space 에 맞게 Policy 와 Critic 의 출력 구조가 변경되었다.
  - Reparameterization Trick 과 같이 Continuouse Action Space 를 가정하고 사용된 부분들이 대체되었다.

### Hyperparameter

- **tau**: Target network의 soft update를 위한 계수이다.  tau 값이 작을수록 target network가 천천히 업데이트되어 학습이 안정적이지만 느려지고, 클수록 빠르게 업데이트되지만 불안정해질 수 있다.
- **entropy_coef**: Temperature parameter α의 초기값 또는 고정값으로, exploration의 강도를 결정한다.

### Reference

1. [Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (n.d.). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.](<https://arxiv.org/pdf/1801.01290>)
2. [Haarnoja, T., Aurick, Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar, V., Zhu, H., Gupta, A., Abbeel, P. and Levine, S. (n.d.). Soft Actor-Critic Algorithms and Applications.](<https://arxiv.org/pdf/1812.05905>)
3. [Christodoulou, P. (2019). Soft Actor-Critic for Discrete Action Settings.](<https://arxiv.org/pdf/1910.07207>)
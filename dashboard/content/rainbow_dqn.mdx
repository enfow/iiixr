---
title: "Rainbow DQN Algorithm"
date: 2024-01-16
type: "model"
---

Rainbow DQN은 DQN의 성능을 개선하기 위해 제안된 다양한 기법들을 한 번에 적용하는 알고리즘이다. 주요 기법들로는 Q value의 Over Estimation 문제 해결을 목적으로 앞선 알고리즘에서도 많이 사용된 Double DQN, Temporal Differene 가 높은 transaction 을 학습에 사용할 확률을 높이는 Prioritized Experience Replay buffer, state value 와 advantage value 를 각각 다른 네트워크로 학습하여 정확도를 높이는 Dueling DQN, 1-step return 이 아닌 n-step return 을 사용하여 long term reward를 고려하게 하는 multi-step bootstrap, 단순히 Q value 가 아닌 그것의 분포를 학습하여 보다 정확한 근사를 가능하게 하는 Distributional RL(C51), parameter space에서 noise를 추가하여 structured exploration을 수행하는데에 도움을 주는 Noisy Network 등이 있다.

DQN 계열의 알고리즘으로, Discrete Action Space 를 가진 환경에 대해서만 적용이 가능하지만, 다양한 기법들이 서로 결합하여 높은 안정성과 성능을 보여주는 알고리즘이라 할 수 있다.

### Rainbow DQN hyperparameter

- **target_update_interval**:  Target network를 업데이트하는 step 수를 의미한다. interval 마다 target network 를 main network의 파라미터로 만들어준다.
- **alpha**:  Prioritized experience replay에서 priority의 강도를 조절하는 parameter 이다. 1 은 priority 에 따른 샘플링(greedy)을, 0은 uniform sampling 을 의미한다. priority 순서대로 샘플링하게 되면 overfitting 의 가능성이 높아져, 보통 0.6 정도의 값을 사용한다.
- **beta_start**: priority에 따라 샘플링을 하면 bias 가 생기게 되는데, beta 는 이를 보정하는 역할을 하며, beta_start 는 그 시작 값을 의미한다. 학습이 진행됨에 따라 그 크기가 지속적으로 커진다.
- **beta_frames**: beta 가 beta_start 에서 1.0 에 도달하는 데에 소요되는 frame 수(step)이다.
- **n_steps**: Multi step bootstrap 에서 step 수를 의미한다. 크면  클수록 더욱 긴 reward 를 반영할 수 있지만 variance 와 연산량이 커진다.
- **n_atoms**: Distributional RL 에서 category 의 개수를 뜻하며, 51을 주로 사용한다(c51).
- **v_min**, **v_max**: value distribution 의 최대 최소 범위를 설정하는 값으로, 환경에서 가능한 최대 최소의 return 범위로 설정한다. 따라서 환경에 맞게 적절히 설정해야 한다.